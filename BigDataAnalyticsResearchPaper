Definition and Implementation of Big Data Analytics Assignment 1
Christian Maxfield Welshinger
Dr. Jun Liu
05/28/2024
 
Definition and Implementation of Big Data Analytics Assignment 1

Big Data Synopsis

In recent years, the amount and variety of data that is constantly generated has necessitated innovations in computer machinery, novel software applications, and a distinct set of skills and techniques in order for such data to be managed, stored, and processed. This deluge of data with which we are concerned is so large, and encompasses so many different formats, with such a breadth of dimensions, that it is fundamentally different from traditional data as it has been previously understood. It is for this reason that a new term has been coined: Big Data. Although some may feel that this name is misleading, what is certain is that Big Data is something truly unique, demanding an entirely new slew of technology and approaches to understand it and make use of it. The reason for which some may take issue with this name is that it could incorrectly lead one to believe that Big Data is simply analogous to a very big table, one which has surpassed some arbitrary threshold in the number of rows it contains, thereby making it “Big”. This, however, is a gross misunderstanding of the term. “Volume” is only one characteristic that determines what makes Big Data. There are in fact 5 criteria, the first of which is volume, the others being variety, velocity, veracity, and value (Zikopoulos et al., 2012). Although the variety component of Big Data comprises the notion of having a great number of dimensions or variables, like many columns in a table, more importantly, variety refers to data that includes objects not tabular: images, videos, sounds, text compositions, and other forms of unstructured or semi-structured data. Obviously, these mediums of digital information are very large in size, but they also require distinct software platforms and applications to analyze them. The velocity of Big Data refers to a few characteristics. It is with great speed that quantities of data are generated and flow into a data collection site, but more critically, in the context of Big Data analysis, velocity refers to the speed with which one can gain insight from data. Utilizing a Big Data platform is not simply about accomplishing the technical feat of storing terabytes of data, rather, it is about carrying out analysis on large quantities of data while the data still has relevancy. Veracity of Big Data refers to its trustworthiness. Obviously, with any data, big or small, trustworthiness is a factor. Yet with huge amounts of data, comes an essentially unique challenge of dealing with large amounts of noise. This noise can broadly be distinguished into two categories: data that is bad (incorrect, corrupted in some way) and data that is irrelevant (not useful in answering pertinent questions related to business initiatives). In other words, there is a uniquely difficult challenge to ensure that Big Data both correctly answers a given question, and answers the actual question that one is asking.
In the world of business and in the context of academic research, Big Data presents a new paradigm of challenges and requirements, but also new opportunities. Companies can better predict the purchasing behavior of their clientele, and proactively solve problems by making new kinds of insights accessible in ways that are relevant at every level of an enterprise, from the customer service department forecasting call volume, to the marketing department better targeting a population for an advertising campaign.
In regards to the possibilities that Big Data opens up for research, entire populations can now be studied in certain cases, instead of relying on sampling. Researchers studying something as multi-faceted as human health can include a much broader set of variables, and with a much larger set of data with which to build predictive models.

Case Studies

In China, where there have been new requirements and laws instituted regarding energy consumption, a study involving Internet of Things (IoT) devices was conducted with the goal of improving the efficiency of energy use, particularly in a residential context (Li, 2024). In the study, sensors were placed around the home that would generate data based on readings of heat and pressure in various rooms, in and around the heating components, and in the areas connecting them. Data was preprocessed and sent to the cloud for analysis and storage. Data picked up by the sensors relayed information about the heat storage heating system, user behavior, indoor environmental conditions, exterior conditions, and other data.
Twenty-five boilers of a heating enterprise were monitored for their use of energy. The boiler rooms were monitored for fuel consumption, water supply temperature, water supply flow, power supply and other factors, all with IoT technology. Reports on the usage of the boilers were generated frequently, making it possible to regularly correlate the effect of the IoT devices in the civil buildings with real-time energy usage. The advantages of IoT data over traditional methods of obtaining data in civil buildings includes improved data validity, data efficiency, and data intuitiveness. That is to say, with IoT devices, there is reduced human interference in the readings, a regularized and monitored frequency of the readings, and the possibility of data visualization relating to HVAC systems and their usage. After analysis and processing of data in the cloud, data is presented to the user in text form. Ultimately, the data generated by these sensors directed the users in making better decisions as to when and where and at what temperature to set and operate the heating elements. Current heating systems in the area of interest in the study were lacking in the ability to provide on-demand and uniform heating. With the application of IoT devices and the resulting data, a synergy of improving performance and
efficiency was realized. This resulted in a more even coverage of heat throughout the home. The project resulted in a 4% decrease in electrical energy, and a 1% decrease in consumption of natural gas. In the realm of healthcare, Big Data is being used to improve clinical decision making, to restructure health care organizational components, and to identify correlations between asynchronous events. The concept of the “data, informational and knowledge” spectrum has formally been rebranded in the health care sector as the Learning Health Care System Cycle, in which health care intervention and practice are intertwined with research and analysis (Rolla, 2024). In this approach, clinical practice and research support and inform each other, helping to reveal underlying patterns and unearthing hidden correlations.
During the delivery of health care, EHRs (Electronic Health Records) are generated and sent to a database for further analysis. The electronic documents are a major source of data in health care, which since the passing of the Health Information Technology Act of 2009, have been widely adopted in acute care hospitals, and have also been more broadly adopted across the globe. In regards to analytics, these records are used to identify and link patients in the real world with relevant clinical studies, and similarities between patients can be discovered and quantified from a number of techniques, including deep learning, natural language processing, tensor factorization and other machine learning and AI tools, and integrated with data from textual sources. Process information from administrative databases is also used to further enrich the data. This large amount of EHR data can also be joined with large datasets relating to medical conditions, medications, and treatment approaches. Additionally, IoT biosensor devices and fitness applications are generating data used to analyze patient health. This has resulted in a huge amount of medical data, and the adoption of Big Data analytics is helping to achieve the ultimate
objective of creating a data infrastructure of real-time knowledge that helps to predict patient events, prevent illness, and personalize care.

Practical Guidelines for Analytics Implementation

The overarching theme of the report relates to data analytics adoption in companies with varying challenges and levels of experience in the domain. The greatest difficulties in adopting analytics for many companies relate to management investing the resources necessary, and the difficulty of changing systems, processes, governance, and existing “ways of doing things” (LaValle et al., 2024). In order to successfully implement analytics, there must be a reasonably short time between the implementation, and value derived therefrom. Additionally, this implementation needs to consist of changes that are manageable in scope and incremental. The following are a few bits of advice for companies looking to adopt or to improve their analytics capabilities. First, a company should identify its biggest challenge as an initial project. This ensures that management will be willing to make the necessary investment. By upping the stakes, one can force a “all hands on deck” approach, even when it upsets the status quo. Secondly, when implementing a new system of analytics, it is important to start not with the data, but with questions that need answers. This avoids spending time cleaning and preparing data that will ultimately end up being irrelevant. Thirdly, analytics implementation should be additive in nature. This means that it is ideal for new analytics systems to be added to any existing systems, instead of immediately replacing them. This also means that a system first implemented in one specific department or line of business will ideally spread out to other company sectors.
I am in agreement that the “question first, data second” approach is a good principle to follow, especially in the context of Big Data, where it would certainly be possible to spend substantial time preparing data to answer irrelevant questions. One nuance, however, that I may propose: it seems reasonable to generally start an analytics initiative with the biggest problem a company has, but it may be the case that the biggest problem could be too difficult or too large in scope as a first project. Therefore, there may be times when a company should settle for a more manageable, albeit important, problem as the first undertaking in their data analytics evolution.


References

LaValle., Lesser, E., Shockley, R., Hopkins, M., Kruschwitz, N. (2011). Big Data, Analytics and the Path From Insights to Value.
MITSloan Management Review, 52(2). https://sloanreview.mit.edu/article/big-daa-analytics-and-the-path-from- insights-to-value/?switch_view=PDF

Li, M. (2024). Big Data Information Collection of IoT For Heat Storage Heating Control System.
Thermal Science, 28(2B), 1303-1311.
10.2298/TSCI2402303L

Rolla, K. (2023). Trends and Futuristic Applications of Big Data and Electronic Health Record Data in Empowering Constructive Clinical Decision Support Systems. Bio Science Research Bulletin-Biological Sciences, 39(2), 78-91. 10.48165/bpas.2023.39.2.6

Zikopoulos, P., deRoos, D., Parasuraman, K., Deutsch, T., Giles, J., & Corrigan, D. (2012). Harness the Power of Big Data The IBM Big Data Platform. McGraw Hill Professional.
